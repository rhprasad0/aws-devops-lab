# =============================================================================
# Scale Test Deployment - Week 12 Task 4
# =============================================================================
# This deployment creates 10 replicas with resource requests designed to
# trigger Karpenter node provisioning. Each pod requests 200m CPU and 256Mi
# memory, which will exceed the capacity of the existing managed nodes.
#
# Usage:
#   kubectl apply -f k8s/karpenter/scale-test-deployment.yaml
#   kubectl get pods -n scale-test -w
#   kubectl get nodes -w
#
# Watch Karpenter logs:
#   kubectl logs -n kube-system -l app.kubernetes.io/name=karpenter -f
#
# Clean up:
#   kubectl delete namespace scale-test
# =============================================================================
apiVersion: v1
kind: Namespace
metadata:
  name: scale-test
  labels:
    purpose: karpenter-testing
    week: "12"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: inflate
  namespace: scale-test
  labels:
    app: inflate
    purpose: karpenter-testing
spec:
  replicas: 10
  selector:
    matchLabels:
      app: inflate
  template:
    metadata:
      labels:
        app: inflate
    spec:
      # Ensure pods land on Karpenter-managed nodes (not the managed node group)
      # This affinity prefers nodes with the karpenter.sh/nodepool label
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: karpenter.sh/nodepool
                    operator: Exists
      # Tolerate all taints so pods can schedule on any node
      tolerations:
        - operator: Exists
      containers:
        - name: inflate
          # pause container - minimal resource usage, just holds the pod running
          image: public.ecr.aws/eks-distro/kubernetes/pause:3.9
          resources:
            requests:
              # Each pod requests 200m CPU (0.2 cores) and 256Mi memory
              # With 10 replicas: 2 CPU cores and 2.5Gi memory total
              # This should exceed available capacity on 2x t4g.medium nodes
              cpu: "200m"
              memory: "256Mi"
            limits:
              cpu: "500m"
              memory: "512Mi"
      # Spread pods across nodes for better distribution
      topologySpreadConstraints:
        - maxSkew: 2
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app: inflate

